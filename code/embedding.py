#%% Import libraries

import sys
import os
import time
import gc

import pandas as pd
import numpy as np
import tensorflow as tf
import multiprocessing as mp
import itertools as it

from datetime import datetime
from tensorflow.keras.callbacks import CSVLogger

# Project specific modules
from lib import io_file_handling as io
from lib import preprocessing as pp
from lib import models


#%% Tune CPU for better tensorflow parallelization
# Even if the model is run on GPU, these parameters reduce the CPU bottleneck during training
# https://www.intel.com/content/www/us/en/developer/articles/technical/effectively-train-execute-ml-dl-projects-on-cpus.html
# Additionally, switch to Tensorflows's graph mode, instead of eager, as the latter heavily leaks memory during Keras' model.fit()

os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'

tf.config.threading.set_intra_op_parallelism_threads(16)
tf.config.threading.set_inter_op_parallelism_threads(2)
tf.config.set_soft_device_placement(enabled = True)
tf.compat.v1.disable_eager_execution()

os.environ["OMP_NUM_THREADS"] = "16"
os.environ["KMP_BLOCKTIME"] = "30"
os.environ["KMP_SETTINGS"] = "1"
os.environ["KMP_AFFINITY"]= "granularity = fine, verbose, compact, 1, 0"


#%% Define main class

class Embedding():
    
    """
    The Embedding class encapsulates the training of both the NSG (using the 
    negative sampling skip-gram version of word2vec) and the NTX (using 
    normalized temperature-scaled cross entropy) embedding models.

    ...
    
    Args
    ---------
    input_settings: json object from the input file, using json.load()
        The input JSON file defines the location of the necessary source files, 
        the path to the output directory and the changeable hyperparameters for
        the training. See the template JSON and the class attributes for 
        further details.

    Attributes
    ----------
    input_settings: json object, see in Args
    
    main_data_path: str
        Full path to the main input data file (CSV), i.e. the context-sampled
        report corpus files, containing a two-columned list (delimiter '|', 
        no header) of target-context term pairs. The elements of this list are 
        not unique. This file is analogous to a skip-gram sampled corpus with 
        word level tokenization. Here, the skip-gram "window" was the adverse 
        event reports, while words were the DrugBank names of drugs and MedDRA 
        Preferred Terms of reactions. Depending on the input file, target terms
        are limited to reactions and context terms are determined by the 
        context-sampling method, i.e.: drugs for reaction2drug, reactions for 
        reaction2reaction, and both for reaction2all. Both models learn 
        embedding vectors for all terms, but they are not designed for
        optimizing the representation of the context terms the same way they do
        with the target terms.
        
    target_freq_path: str
        Full path to the target term frequency file (CSV), containing a two-
        columned list (delimiter '|', no header) of target-freaquency pairs.
        The frequency, or more precisely, the number of occurrences (int) of 
        each target term was calculated based on the entire report corpus,
        before rare terms under a threshold (defined during preprocessing, 11 
        in our case) were removed. This file's main purpose is to serve as 
        the basis for the NSG model, which uses the target term frequencies 
        for subsampling the corpus (which balances the occurrence of terms with
        varying frequency in the training) and also uses the context term
        frequencies to select negative pairs to each positive. The NTX model 
        uses it only opitonally, as in default it samples (positive) pairs 
        uniformly for the given batch.
        
    context_freq_path: str
        Same as target_freq_path, only for context terms.
    
    embedding_method: str
        Possible values: ntx, nsg
        This parameter will determine which model, loss and training paradigm
        will be used to create the reaction representations. It also influences
        the computation time, as the NSG model takes significantly longer to
        train.
        
        nsg:
            This method is based on the negative sampling skip-gram version of
            word2vec model, using additional subsampling. It's implemented as a
            shallow neural network that performs classification of target-
            context term pairs by first embedding them in the given dimension, 
            then calculates their dot product and applies non linearity 
            (sigmoid). For each "positive" term pair, a number of randomly 
            sampled "negative" pairs are generated by randomly sampling new 
            context terms for the given target term, following the smoothed
            unigram distribution of context terms. The corresponding positive-
            negative pairs are fed to the model in the same batch. The quality 
            of resulting embedding vectors rely on the noise-contrastive 
            estimation methodology, as the "fake" context terms are too random 
            and so, can be considered as noise, while the "real" context terms
            will appear consistently and pull the vectors out of the noise, 
            towards their optimal position in the embedding space.
            
        ntx:
            This method is based on contrastive pretraining models from
            computer vision, such as SimCLR. It's implemented as a slightly
            deeper neural network compared to the NSG model. Here, after the
            initial embedding of the input terms, the vectors go through a
            shared projection head (2 fully connected layers with ReLU between)
            then the model performs a two-way multi-class classification on all
            the term pairs in the batch, considering the given context for 
            each target to be the right class and all other context terms in 
            the batch to be wrong, then the same for the context terms against 
            all target terms. In practice, a similarity matrix is constructed 
            among the target-context pairs, down-scaled with the temperature
            parameter, where cross-entropy is calculated as the loss, using all
            columns, then all rows against the corresponding columns and rows
            of the identity matrix as the expected outcome. During 
            preprocessing, for each unique target term, a uniformly random 
            context term from its corresponding list of context terms is 
            sampled in such a way, that a number of (batch size) consecutive 
            target-context term pairs will contain no duplicates in either 
            term types. These consecutive non-duplicate pairs will correspond 
            to the batches during training.
            
    norm_vecs: boolean
        This parameter determines whether the internal term representations 
        inside the neural networks are to be normed or not. In the NTX model,
        it is recommended to norm them (cosine similarity), while the NSG model
        best operates without the norm (dot product).
        
    embedding_dim: int
        The number of hidden neurons that embed the terms as vectors, and
        consequently, the dimension of the resulting term vectors. Based on
        literature and empirical testing, the recommended dimension is between
        100-200. Although it is common practice to use 300 in NLP, those are
        for much larger vocabularies, meaning a higher data complexity, which
        requires higher dimensions to properly approximate.
        
    numof_epochs: int
        The number of times the model goes through the entire dataset during 
        training.
        
        For the NSG model
            As the corpus of adverse event reports is rather large, a single 
            epoch might be sufficient to present acceptable term vectors, and 
            also desirable, as even one epoch might take several hours. 
            However, based on empirical tests, it is recommended to go up to 
            20-25 epochs, and set an appropriate epoch_per_out parameter (see 
            later), which enables multiple results from the various epoch 
            numbers.
            
        For the NTX model
            As the corpus is reduced to only the unique target-context pairs,
            the training of the NTX model contains significantly shorter 
            epochs. Here, for each unique target term, a unique context is
            randomly sampled from its corresponding context set in such a way,
            that the a number of consecutive resulting target-context pairs
            will contain no duplicates, as that would introduce contradicting 
            information in the similarity matrices calculated by the model.
            Consequently, the size of the training dataset will be equal to the
            number of unique target terms, and an epoch will be (number of 
            unique target terms / batch size) long. Based on empirical tests, 
            the training of the NTX model requires more than simply a few 
            epochs, recommended 100-300, but as the training is rather fast, it
            is easy to go up to even 1000.
            
    optimizer: str
        Possible values: rmsprop, sgd, adagrad, adam
        This parameter determines the type of optimizer to be used during model
        training. Based on empirical tests, the rmsprop (Root Mean Squared 
        Propagation) optimizer worked the best and most consistently for both 
        models. While the adam (Adaptive Moment Estimation) optimizer could
        also reach relatively good results, both the adagrad (Adaptive Gradient
        Algorithm) and sgd (Stochasitc Gradient Descent) optimizers seemed to 
        struggle.
        
    learning_rate: float
        The rate of learning passed down to the optimizer. Recommended: 0.01
        
    epoch_per_out: int
        This parameter determines the epoch intervalls at which the internal
        term representations are written to the output directory. For the NSG
        model with 30 epochs, it is recommended to use 3-5, while for the NTX
        model with 500 epoch, it is recommended to use 50-100. Additionally, 
        the training set is rebuilt for the NSG model at each epoch_per_out
        interval, while for the NTX model the entire training set for all 
        epochs is prepared in advance.
    
    frequeny_sampling: boolean
        This is an NTX exclusive parameter, determining whether to sample
        the (positive) context terms into a batch uniformly, or based on their
        smoothed unigram distribution similar to the NSG model. Based on
        empirical testing, enabling frequency-based sampling significantly
        decreases the quality of the resulting vectors. Probably because these 
        sampled positive context terms serve as the negative context for
        other target terms in the batch, for which the samples are not checked 
        whether they are also a positive context for them or not. Thus, highly 
        frequent reactions could appear as incorrect context way too often. To 
        a lesser extent, this might still be a problem for reactions that are 
        not frequent as in an absolut term but simply occur with a lot of 
        drugs.
        
    numof_pairs: int
        This is an NTX exclusive parameter, determining how many target-context
        pairs are sampled without duplicates in a consecutive manner, resulting
        in this parameter also determining the batch size. Recommended 8-32, as
        too large numbers may lead to misshapen training data due to some
        target terms not having enough unique context terms to make it into the
        batches.
        
    temperature: float
        This is an NTX exclusive parameter, determining the down-scaling factor
        (temperature) of the similarity matrix used for calculating cross 
        entropy. The temperature defines the "softness" of the softmax 
        distribution used in the loss. Lower values generally lead to a higher 
        contrastive accuracy. Recommended value: 0.1-0.3
    
    numof_poss: int
        This is an NSG exclusive parameter, determining the number of positive
        target-context pairs in one batch. See numof_negs for further details.
        
    numof_negs: int
        This is an NSG exclusive parameter, determining the number of negative
        context terms randomly sampled for each positive target-context 
        according to the smoothed unigram distribution of context terms in
        a way that the random context term does not match the original positive 
        one. As a result, a batch of training data will contain numof_poss 
        number of positive target-context terms from the corpus and (numof_poss
        * numof_negs) number of random negative pairs. Consequently, the batch 
        size for the training is given by (numof_poss * (numof_negs +1)). The
        recommended value for numof_negs is 5-7, and for numof_poss it is
        recommended to make the eventual batch size into a power of 2, e.g.:
        with numof_negs = 7 and numof_poss = 8.
    
    output_path: str
        Full path to the output directory (ending in '/'), where a new folder 
        will be created, named by the project_name + a time stamp.
        
    project_name: str
        Used for the creation of the output folder, can be empty.
    
    out_dir: str
        The output folder used to print out the result files. See project_name
        and output_path for further details.
        
    main_data: pandas dataframe
        Serves as the main corpus data, see main_data_path for further details.
        
    target_freq: dictionary
        Contains the target terms as keys and their number of occurrences in 
        the corpus as values. See target_freq_path for further details.
        
    context_freq: dictionary
        Contains the context terms as keys and their number of occurrences in 
        the corpus as values. See context_freq_path for further details.
        
    target_dict_i: dictionary
        A simple supporting dictionary for efficient look-up of the indices of
        the target terms, that are used as input integers for the models.
        
    target_dict_s: dictionary
        Contains the subsampling values of the target terms, which determine
        the probability of them being included in the training set each time
        they appear in the corpus. These probabilities are in inverse
        correlation with the frequencies of terms, so that the more frequent
        a target term is, the less chance it has to be included. Under a
        certain frequency, they are always included. This is required due to
        the extreme differences of the number of occurrences observed in the 
        corpus.
        
    context_dict_i: dictionary
        A simple supporting dictionary for efficient look-up of the indices of
        the context terms, that are used as input integers for the models.
        
    context_dict_u: dictionary
        Contains the smoothed unigram distribution (down-scaled and normalized
        by a factor of ^3/4) of the context terms. It is used during the
        sampling of the random negative context terms for the NSG model, and
        optionally for the sampling of positive context terms for the NTX
        model, when frequeny_sampling = True.
        
    pre_data: dictionary (of dictionaries)
        Created from main_data, to be used for the preparation of the training 
        dataset for the NTX model. Here, target term indices are the top level
        keys, and dictionaries of corresponding context indices are the values.
        
    model: tf.keras.Model() object
        The constructed neural network model based on the input 
        hyperparameters. A simplified visualization of its structure is 
        generated in the out_dir by graphviz.
        

    Methods
    ----------
    read_settings():
        Extracts the input parameters from the provided json object into class
        variables. Furthermore, creates the output folder at the designated
        destination. Executed during class initialization. See out_dir and the 
        rest of the parameter descriptions for further details.

    train():
        The main method of the Embedding class, intended to start the entire
        process.
        
    read_files():
        Reads into memory the 3 input data files provided, see main_data_path,
        target_freq_path and context_freq_path for further details.
        
    prepare_dictionaries():
        Generates the supporting dictionaries (target_dict_i, target_dict_s,
        context_dict_i, context_dict_s), maps them into the main_data dataframe
        and optionally creates the pre_data dictionary, when the embedding
        method is set to ntx.
        
    build_model():
        Builds and compiles the neural network model according to the input
        parameters, then calls visualize() to generate a simplified PNG image 
        of its structure.
        
    visualize():
        Prints a summary onto the console and a simplified PNG image in the 
        output folder using graphviz.
        
    train_ntx():
        Initiates the preprocessing of the training data, then commences the
        training of the NTX model, from which the results are periodically
        generated in the output folder. See epoch_per_out for further details.
        
    train_nsg():
        Initiates the preprocessing of the training data, then commences the
        training of the NSG model, from which the results are periodically
        generated in the output folder. The training data is rebuilt at given
        intervals. See epoch_per_out for further details.
        
    print_results(epoch):
        Called by train_ntx() and train_nsg(), responsible for saving both the
        target and context vector representations in a separate CSV file
        (delimiter '|', no header), where the first column is the embedded term
        itself and the rest of the columns contain the floating point values of
        the corresponding vector. The input parameter 'epoch' is used for the 
        file name generation.
        
    get_input_data_for_ntx():
        Initiates the parallelized (by python multiprocessing) preparation of 
        the training data for the NTX model, which generates the data for all
        epochs in advance. Returns a numof_epochs long list of two-element 
        numpy arrays containing integers corresponding to the indices of 
        target-context term pairs.
        
    get_input_data_for_nsg():
        Initiates the parallelized (by python multiprocessing) preparation of 
        the training data for the NSG model. This method is called again and
        again at each epoch_per_out interval. Returns a list of three-element
        numpy arrays, containing the integers of the corresponding target-
        context term pairs and their assigned label (0 or 1). This list is then 
        concatenated into a 2D numpy array in train_nsg().
        
    get_optimizer():
        Selects and returns a tf.keras.Model() compatible optimizer object,
        based on the input parameters.
        
    get_logger():
        Creates and returns a tf.keras.Model() compatible CSVLogger callback 
        object, that is used to print out the loss values of the model training
        at the end of each epoch into a CSV file inside the output folder.
        
    get_out_dir():
        Returns the class variable out_dir, which is the full path of the
        generated output folder. Intended to be called outside of the class,
        so the folder can be accessed by other methods.
    

    """
    
    def __init__(self, input_settings):
        self.input_settings = input_settings
        self.read_settings()
        
        
    def read_settings(self):
        print(str(datetime.now()) + ' - Reading input parameters')
        
        # Source files
        self.main_data_path = self.input_settings["oSourceList"]["oMainData"]
        self.target_freq_path = self.input_settings["oSourceList"]["oTargetFrequencyFile"]
        self.context_freq_path = self.input_settings["oSourceList"]["oContextFrequencyFile"]
        
        # General parameters
        self.embedding_method = self.input_settings["oEmbeddingParams"]["oEmbeddingMethod"]
        self.norm_vecs = self.input_settings["oEmbeddingParams"]["oNormalizeVectorLength"]
        self.embedding_dim = self.input_settings["oEmbeddingParams"]["oNumberOfEmbeddedFeatures"]
        self.numof_epochs = self.input_settings["oEmbeddingParams"]["oEpochs"]
        self.optimizer = self.input_settings["oEmbeddingParams"]["oOptimizer"]
        self.learning_rate = self.input_settings["oEmbeddingParams"]["oLearningRate"]
        self.epoch_per_out = self.input_settings["oEmbeddingParams"]["oEpochsPerOutput"]
        
        # Ntx specific parameters
        self.frequeny_sampling = self.input_settings["oEmbeddingParams"]["oNtXentExclusive"]["oSampleByFrequency"]
        self.numof_pairs = self.input_settings["oEmbeddingParams"]["oNtXentExclusive"]["oNumberOfPairsPerBatch"]
        self.temperature = self.input_settings["oEmbeddingParams"]["oNtXentExclusive"]["oContrastiveTemperature"]
        
        # Nsg specific parameters
        self.numof_poss = self.input_settings["oEmbeddingParams"]["oNegativeSamplingExclusive"]["oNumberOfPositivesPerBatch"]
        self.numof_negs = self.input_settings["oEmbeddingParams"]["oNegativeSamplingExclusive"]["oNumberOfNegativesPerPositive"]
        
        # Output path
        self.output_path = self.input_settings["oOutputPath"]
        self.project_name = self.input_settings["oProjectName"].lower()
        
        self.out_dir = io.make_out_dir(self.output_path, self.project_name)
        
        
    def train(self):
        
        self.read_files()
        self.prepare_dictionaries()
        self.build_model()
        
        if self.embedding_method == 'ntx':
            self.train_ntx()
        
        elif self.embedding_method == 'nsg':
            self.train_nsg()
        
        
    def read_files(self):
        print(str(datetime.now()) + ' - Reading input files')
        
        self.main_data = pd.read_csv(self.main_data_path, delimiter = '|', names = ['TARGET', 'CONTEXT'], header = None)
        self.target_freq = io.read_simple_dict(self.target_freq_path, '|', False)
        self.context_freq = io.read_simple_dict(self.context_freq_path, '|', False)
        
        
    def prepare_dictionaries(self):
        print(str(datetime.now()) + ' - Preparing dictionaries')
        
        self.target_dict_i, self.target_dict_s = pp.build_dict_with_subsamp_dist(self.main_data, self.target_freq)
        self.context_dict_i, self.context_dict_u = pp.build_dict_with_unigram_dist(self.context_freq)
        
        self.main_data['TINDEX'] = self.main_data['TARGET'].map(self.target_dict_i)
        self.main_data['CINDEX'] = self.main_data['CONTEXT'].map(self.context_dict_i)
        self.main_data['SUBS'] = self.main_data['TARGET'].map(self.target_dict_s)
        
        if self.embedding_method == 'ntx':
            self.pre_data = self.main_data.groupby('TINDEX').apply(
            lambda x: x.groupby('CINDEX').count()['TINDEX'].to_dict()).to_dict()
        
    
    def build_model(self):
        print(str(datetime.now()) + ' - Building model')
        
        if self.embedding_method == 'ntx':
            self.model = models.build_ntx(self.target_dict_i, self.context_dict_i, self.embedding_dim)
            self.model.compile(loss = models.calc_ntx_loss(norm_vecs = self.norm_vecs, temperature = self.temperature), 
                                            optimizer = self.get_optimizer())
            
        elif self.embedding_method == 'nsg':
            self.model = models.build_nsg(self.target_dict_i, self.context_dict_i, self.embedding_dim, self.norm_vecs)
            self.model.compile(loss = 'binary_crossentropy', optimizer = self.get_optimizer())
            
        else:
            sys.exit("[Error] - valid embedding methods are: ntx, nsg.")
            
        self.visualize_model()
            

    def visualize_model(self):
        print(self.model.summary())
        tf.keras.utils.plot_model(self.model, to_file = self.out_dir + 'model.png',
                              show_shapes = True, show_layer_names = True, rankdir = 'TB', 
                              expand_nested = False, dpi = 96)
        
        
    def train_ntx(self):
        
        # For the ntx model, training data is generated for all epochs in advance
        in_data = self.get_input_data_for_ntx()
        csv_logger = self.get_logger()
        
        for epoch in range(self.numof_epochs):
            
            print(str(datetime.now()) + " - Commencing training of epoch {}".format(epoch + 1))
            self.model.fit([in_data[epoch][:, 0], in_data[epoch][:, 1]], np.zeros((in_data[epoch][:, 0].size)), 
                      batch_size = self.numof_pairs,  callbacks = [csv_logger])
            
            if ((epoch + 1) % self.epoch_per_out == 0 or epoch == 0):
                self.print_results(epoch)
                
        
    def train_nsg(self):
        print(str(datetime.now()) + " - Building initial corpus")
        
        # For the nsg model, the training data is regenerated after a number of epochs
        in_data =  np.concatenate(self.get_input_data_for_nsg(), axis = 0)
        csv_logger = self.get_logger()
        
        for epoch in range(self.numof_epochs):
            
            if (epoch % self.epoch_per_out == 0 and epoch != 0):
                print(str(datetime.now()) + " - Rebuilding corpus for epoch {}".format(epoch + 1))
                del in_data
                gc.collect()
                in_data =  np.concatenate(self.get_input_data_for_nsg(), axis = 0)
            
            print(str(datetime.now()) + " - Commencing training of epoch {}".format(epoch + 1))
            self.model.fit([in_data[:, 0], in_data[:, 1]], in_data[:, 2], 
                      batch_size = self.numof_poss * (self.numof_negs + 1), 
                      callbacks = [csv_logger])
            
            if ((epoch + 1) % self.epoch_per_out == 0 or epoch == 0):
                self.print_results(epoch)
                
                
    def print_results(self, epoch):
        io.print_embeddings(self.model.get_layer('target_embedding'), self.model.get_layer('context_embedding'), 
                            epoch + 1, self.target_dict_i, self.context_dict_i, self.out_dir)
        
        
    def get_input_data_for_ntx(self):
        
        n_cpu_worker = mp.cpu_count()
        main_data = []
        
        # Initiate a multiprocessing pool of size n_cpu_worker
        # Each pool process will produce one epoch worth of training data at a time
        # The returned main_data is a numof_epochs long list of 2D numpy arrays containing
        # the corresponding index integers of the sampled positive target-context pairs.
        # In these numpy arrays, numof_pairs number of consecutive target-context pairs
        # do not contain duplicate terms on either side.
        with mp.Pool(processes = n_cpu_worker) as pool:
        
            main_data.extend(pool.starmap(pp.prep_corpus_ntx, zip(
                                                    it.repeat(self.pre_data),
                                                    it.repeat(self.numof_pairs),
                                                    it.repeat(self.frequeny_sampling),
                                                    range(0, self.numof_epochs))))
            pool.close()
            pool.join()
                            
        return main_data
    
    
    def get_input_data_for_nsg(self):
                    
        n_cpu_worker = round(mp.cpu_count() / 2)
        data_shuffled = self.main_data.sample(frac = 1).reset_index(drop = True)
        
        # Pre-slice the corpus so the mp.Pool workers will have larger chunks to
        # work with, reducing the bottleneck of data accessing. The same behavior
        # can probably be achieved by using the chunksize argument of pool.starmap().
        data_split = np.array_split(data_shuffled, n_cpu_worker * 20)
        context_indices =  list(self.context_dict_i.values())
        context_unigram =  list(self.context_dict_u.values())
        main_data = []
        
        # Initiate a multiprocessing pool of half the size of n_cpu_worker.
        # Each pool process receives a chunk of the corpus, where based on the subsampling probabilities,
        # the positive target-context pairs are either included or not.
        # Then, each included positive pair is matched with numof_negs number of
        # negative sampled pairs, where the context term is sampled following a unigram distribution.
        # The returned flat_list is a list of 3D numpy arrays, containing the correspinding
        # index integers of the sampled target-context pairs and their label, ordered so that positive
        # pairs are followed by their matching negative ones.
        # maxstaskperchild = 1 ensures that the pool workers are not reused, but restarted
        # this introduces a time overhead, but reduces memory consumption
        with mp.Pool(processes = n_cpu_worker, maxtasksperchild = 1) as pool:
        
            main_data.extend(pool.starmap(pp.child_proc_nsg, zip(
                                                    data_split,
                                                    it.repeat(context_indices),
                                                    it.repeat(context_unigram),
                                                    it.repeat(self.numof_negs), 
                                                    it.repeat(len(data_shuffled)),
                                                    range(0, n_cpu_worker * 20)),
                                                    chunksize = 1))
            pool.close()
            pool.join()
                
        flat_list = [item for sublist in main_data for item in sublist]
            
        return flat_list
    
    
    def get_optimizer(self):
        # Legacy versions are needed for graph mode, as of Tensorflow 2.11
        
        if self.optimizer == 'rmsprop':
            optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate = self.learning_rate, 
                                                     rho = 0.9, 
                                                     momentum = 0.0, 
                                                     epsilon = 1e-07, 
                                                     centered = False, 
                                                     name = 'RMSprop')
        elif self.optimizer == 'adagrad':
            optimizer = tf.compat.v1.train.ProximalAdagradOptimizer(
                                                    learning_rate = self.learning_rate,
                                                    initial_accumulator_value = 0.1,
                                                    l1_regularization_strength = 0.0,
                                                    l2_regularization_strength = 0.0,
                                                    use_locking = False,
                                                    name = 'ProximalAdagrad')

        elif self.optimizer == 'adam':
            optimizer = tf.keras.optimizers.legacy.Adam(
                                                    learning_rate = self.learning_rate,
                                                    beta_1 = 0.9,
                                                    beta_2 = 0.999,
                                                    epsilon = 1e-07,
                                                    amsgrad = False,
                                                    name = 'Adam')

        elif self.optimizer == 'sgd':
            optimizer = tf.keras.optimizers.legacy.SGD(
                                                    learning_rate = self.learning_rate, 
                                                    momentum = 0.9, 
                                                    nesterov = True, 
                                                    name = 'SGD')
        else:
            sys.exit("[Error] - valid optimizer parameters are: rmsprop, adagrad, adam, sgd.")
            
        return optimizer
    

    def get_logger(self):
        return CSVLogger(self.out_dir + 'loss.csv', append = True, separator = '|')
    
    
    def get_out_dir(self):
        return self.out_dir
            
            
#%% Initiate main sequence

if __name__ == "__main__":
    
    # Start timer for computation time measurements
    start_time = time.time()
    # Read the provided json file from command-line arguments
    input_settings = io.read_json_input(sys.argv[1])
    # Initialize the main class
    embedding = Embedding(input_settings)
    # Initialization of the class creates the output directory, where the input json file can now be copied
    io.copy_to_out_dir(embedding.get_out_dir(), sys.argv[1], 'input.json')
    # Redirect print() and errors so that it will also write into a log.txt at out_dir
    sys.stdout = io.Logger(sys.stdout, embedding.get_out_dir())
    sys.stderr = sys.stdout
    # Begin model training, which includes data reading and preprocessing
    embedding.train()
    # Conclude
    print(str(datetime.now()) + ' - All done!')
    elapsed_time = time.time() - start_time
    print(str(datetime.now()) + " - Only took %02d seconds!" % (elapsed_time))
